{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MBTI-4ClassModels.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vv1o0TlHqPYg",
        "colab_type": "text"
      },
      "source": [
        "This notebook contains baseline model for personality prediction  using MBTI dataset. \n",
        "\n",
        "The Myers Briggs Type Indicator (or MBTI for short) is a personality type system that divides everyone into 16 distinct personality types across 4 axis:\n",
        "* Introversion (I) – Extroversion (E)\n",
        "* Intuition (N) – Sensing (S)\n",
        "* Thinking (T) – Feeling (F)\n",
        "* Judging (J) – Perceiving (P)\n",
        "\n",
        "In the dataset, there are 8600 rows of data. Each row contains a person's MBTI personality class and the last 50 things that he/she posted in PersonalityCafe Forum. \n",
        "Instead of 16 classes, binary classifiers trained for 4 classes as : I-E, N-S, T-F and J-P"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu-lOIfog8Jj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import libraries \n",
        "import json \n",
        "import numpy as np\n",
        "seed = 123\n",
        "np.random.seed(seed)\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import learning_curve\n",
        "import seaborn as sns \n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras import layers\n",
        "import zipfile\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras.layers import Dense, LSTM, Bidirectional, GRU, SimpleRNN\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.optimizers import Adam\n",
        "from os.path import join\n",
        "import pandas as pd"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9XsjM00hAPz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define constants \n",
        "data_folder = \"/\"\n",
        "raw_data_file = \"mbti_1.csv\"\n",
        "punc_data_file = join(data_folder,\"preprocessed_data_punc_4class.json\")\n",
        "links_data_file = join(data_folder,\"preprocessed_data_links_4class.json\")\n",
        "letter_data_file = join(data_folder,\"preprocessed_data_letters_4class.json\")\n",
        "type_data_file = join(data_folder,\"preprocessed_data_type_4class.json\")\n",
        "prep_data_file = join(data_folder,\"preprocessed_data_all_4class.json\")\n",
        "none_data_file = join(data_folder,\"preprocessed_data_none_4class.json\")\n",
        "\n",
        "n_splits = 5 # number of splits for cross-validation\n",
        "shuffle_flag = True \n",
        "\n",
        "scoring = {'acc': 'accuracy',\n",
        "           'neg_log_loss': 'neg_log_loss',\n",
        "           'f1_micro': 'f1_micro'}\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWGeGaz0hEDH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define functions \n",
        "def read_dataset(filepath):\n",
        "    with open(filepath) as fp:\n",
        "        return json.load(fp)\n",
        "        \n",
        "plt.style.use('ggplot')\n",
        "\n",
        "def plot_history(history):\n",
        "    acc = None \n",
        "    val_acc = None \n",
        "    loss = None \n",
        "    val_loss = None \n",
        "    if \"accuracy\" in history.history.keys():\n",
        "        acc = history.history['accuracy']\n",
        "    elif \"acc\" in history.history.keys():\n",
        "        acc = history.history['acc']\n",
        "    if \"val_accuracy\" in history.history.keys():  \n",
        "        val_acc = history.history['val_accuracy']\n",
        "    if 'loss' in history.history.keys():\n",
        "        loss = history.history['loss']\n",
        "    if 'val_loss' in history.history.keys():\n",
        "        val_loss = history.history['val_loss']\n",
        "    x = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    title = \"\"\n",
        "    if acc:\n",
        "        plt.plot(x, acc, 'b', label='Training acc')\n",
        "        title += \"Training\" \n",
        "    if val_acc: \n",
        "        plt.plot(x, val_acc, 'r', label='Validation acc')\n",
        "        title += \" and validation\"\n",
        "    title += \" accuracy\"\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    title = \"\"\n",
        "    if loss: \n",
        "        plt.plot(x, loss, 'b', label='Training loss')\n",
        "        title += \"Training\"\n",
        "    if val_loss: \n",
        "        plt.plot(x, val_loss, 'r', label='Validation loss')\n",
        "        title += \" and validation\"\n",
        "    title += \" loss\"\n",
        "    plt.title(title)\n",
        "    plt.legend()\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFUMcmv7hJzM",
        "colab_type": "text"
      },
      "source": [
        "## Prepare data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puCtKMd-q3cp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_data_tfidf(data_file, max_features= 5000, json_flag=False):\n",
        "    \"\"\" for a given datafile reads the content and returns \n",
        "      posts and labels \n",
        "      @data_file: string, the path of the file \n",
        "      @json_flag: bool, True if given file is in json format\n",
        "      @max_features: int, number of features in tf-idf\n",
        "      Returns \n",
        "      @X_train_cnt: tf-idf vectors of training posts  \n",
        "      @X_tes_cnt: tf-idf vectors of test posts\n",
        "      @y_train: int, train labels \n",
        "      @y_test: int, test labels\n",
        "    \"\"\"\n",
        "    if json_flag:\n",
        "        dataset = read_dataset(data_file)\n",
        "        posts = list(dataset['posts'])\n",
        "        labels = list(dataset['types'])\n",
        "        X = np.array(posts)\n",
        "        y = np.array(labels)\n",
        "    else:\n",
        "        dataset = pd.read_csv(data_file)\n",
        "        posts = list(dataset['posts'].values)\n",
        "        labels = list(dataset['types'].values)\n",
        "        X = np.array(dataset['posts'].values)\n",
        "        y = np.array(dataset[['I-E','S-N','T-F','J-P']].values)\n",
        "\n",
        "    print(\"The number of data %d \" %(len(posts)))\n",
        "    print(\"Example from dataset\")\n",
        "    print(posts[0][0:50]) \n",
        "    print(\"For type %s\" %(labels[0]))\n",
        "    posts = [p.lower() for p in posts]\n",
        "    #return posts, labels \n",
        "\n",
        "    # train-test split \n",
        "\n",
        "    X_train, X_test, y_trains, y_tests  = train_test_split(X, y, \n",
        "                                                    test_size=0.2, \n",
        "                                                    random_state=seed)\n",
        "    \n",
        "    print(\"Number of examples in train set %d \" %(len(X_train)))\n",
        "    print(\"Number of examples in test set %d \" %(len(X_test)))\n",
        "\n",
        "    # create a matrix of numbers to represent posts\n",
        "    tfidf2 = CountVectorizer(ngram_range=(1, 1), \n",
        "                         stop_words='english',\n",
        "                         lowercase = False, \n",
        "                         max_features = max_features)\n",
        "\n",
        "    X_train_cnt = tfidf2.fit_transform(X_train)\n",
        "    X_test_cnt = tfidf2.transform(X_test)\n",
        "    \n",
        "    \n",
        "    return X_train_cnt, X_test_cnt, y_trains, y_tests"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7bAEFu1uKZe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_data(data_file, \n",
        "                 max_sent_len=100, \n",
        "                 json_flag=False,\n",
        "                 padding_place='post',\n",
        "                 max_nb_words = 50000,\n",
        "                 test_size=0.2):\n",
        "    \"\"\" for a given datafile reads the content and returns \n",
        "      posts and labels \n",
        "      @data_file: string, the path of the file \n",
        "      @json_flag: bool, True if given file is in json format\n",
        "      @max_features: int, number of features in tf-idf\n",
        "      Returns \n",
        "      @X_train_cnt: tf-idf vectors of training posts  \n",
        "      @X_tes_cnt: tf-idf vectors of test posts\n",
        "      @y_train: int, train labels \n",
        "      @y_test: int, test labels\n",
        "    \"\"\"\n",
        "    if json_flag:\n",
        "        dataset = read_dataset(data_file)\n",
        "        posts = list(dataset['posts'])\n",
        "        labels = list(dataset['types'])\n",
        "    else:\n",
        "        dataset = pd.read_csv(data_file)\n",
        "        posts = list(dataset['posts'].values)\n",
        "        labels = list(dataset['types'].values)\n",
        "\n",
        "    print(\"The number of data %d \" %(len(posts)))\n",
        "    print(\"Example from dataset\")\n",
        "    print(posts[0][0:50]) \n",
        "    print(\"For type %s\" %(labels[0]))\n",
        "    posts = [p.lower() for p in posts]\n",
        "    #return posts, labels\n",
        "\n",
        "    # get the vocabulary \n",
        "    vocabulary = []\n",
        "    for p in posts:\n",
        "        for word in p.split():\n",
        "            vocabulary.append(word)\n",
        "        if len(p.split())> max_sent_len:\n",
        "            max_sent_len = len(p.split())  \n",
        "    vocabulary = set(vocabulary)\n",
        "    print(\"The vocabulary size is  %d \" %(len(vocabulary)))\n",
        "    print(\"The maximum post length is %d \" %(max_sent_len))\n",
        "\n",
        "    tokenizer = text.Tokenizer(num_words=max_nb_words,\n",
        "                               lower=True,\n",
        "                               split=\" \")\n",
        "    tokenizer.fit_on_texts(posts)\n",
        "    word_index = tokenizer.word_index\n",
        "    print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "    tokenized_X = tokenizer.texts_to_sequences(posts)\n",
        "    print(\"Example from tokenized X \")\n",
        "    print(tokenized_X[0][0:50])\n",
        "\n",
        "    padded_X = sequence.pad_sequences(tokenized_X,\n",
        "                                      maxlen=max_sent_len, \n",
        "                                      padding=padding_place)\n",
        "    print('Shape of data tensor after tokenization and padding:', padded_X.shape)\n",
        "\n",
        "    vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
        "    print(\"Vocabulary Size %d \" %(vocab_size))\n",
        "\n",
        "    # train-test split for deep learning methods\n",
        "\n",
        "    #y_binarized_labels = to_categorical(labels)\n",
        "\n",
        "    X_train_pad, X_test_pad, y_train_pads, y_test_pads = train_test_split(padded_X,\n",
        "                                                                        labels, \n",
        "                                                                        test_size=test_size, \n",
        "                                                                        random_state=seed)\n",
        "    print(\"Number of examples in train set %d \" %(len(X_train_pad)))\n",
        "    print(\"The shape of X training tensor (%d,%d) \" %(X_train_pad.shape))\n",
        "    print(\"The shape of y training tensor (%d) \" %(len(y_train_pads)))\n",
        "\n",
        "    print(\"Number of examples in test set %d \" %(len(X_test_pad)))\n",
        "    print(\"The shape of X test tensor (%d,%d)\" %(X_test_pad.shape))\n",
        "    return X_train_pad, X_test_pad, y_train_pads, y_test_pads , vocab_size, tokenizer"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euz0udt14ip2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def overall_accuracy(y_preds_all,y_tests):\n",
        "  # reverse the numbers as letters \n",
        "  # if types start I -> 1, if starts with E->0 \n",
        "  # if types start S -> 1, if starts with N->0 \n",
        "  # if types start T -> 1, if starts with F->0 \n",
        "  # if types start J -> 1, if starts with P->0 \n",
        "  predictions = []\n",
        "  true_y = []\n",
        "  model_names = ['IE','SN','TF', 'JP']\n",
        "\n",
        "  # y_tests = [y_test_ie, y_test_sn, y_test_tf, y_test_jp]\n",
        "\n",
        "  for i in range(4):\n",
        "    y_preds_sub = y_preds_all[i]\n",
        "    y_preds_letter = np.empty(len(y_preds_sub), dtype=object)\n",
        "    \n",
        "    y_preds_letter[y_preds_sub>=0.5] = model_names[i][0]\n",
        "    y_preds_letter[y_preds_sub<=0.5] = model_names[i][1]\n",
        "\n",
        "    y_true_letter = np.empty(len(y_tests[:,i]),dtype=object) \n",
        "    y_true_letter[y_tests[:,i]==1] = model_names[i][0]\n",
        "    y_true_letter[y_tests[:,i]==0] = model_names[i][1]\n",
        "\n",
        "    predictions.append(y_preds_letter)\n",
        "    true_y.append(y_true_letter)\n",
        "\n",
        "  # concate the class names \n",
        "  predictions = pd.DataFrame(predictions).transpose()\n",
        "  predictions['type'] = predictions.agg(lambda x: f\"{x[0]}{x[1]}{x[2]}{x[3]}\", axis=1)\n",
        "  true_y = pd.DataFrame(true_y).transpose()\n",
        "  true_y['type'] = true_y.agg(lambda x: f\"{x[0]}{x[1]}{x[2]}{x[3]}\", axis=1)\n",
        "  return sum(true_y['type'] == predictions['type'])/len(true_y)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9ELocWBs2B6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "844d167a-881c-4d00-eacc-abefe3483e7a"
      },
      "source": [
        "X_train_cnt, X_test_cnt, y_trains, y_tests = prepare_data_tfidf(letter_data_file)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of data 8675 \n",
            "Example from dataset\n",
            "  http   www youtube com watch v qsxhcwe krw http \n",
            "For type INFJ\n",
            "Number of examples in train set 6940 \n",
            "Number of examples in test set 1735 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0vy6ZyJuquB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "outputId": "6cc7219e-530c-4833-e370-b7e772eace3e"
      },
      "source": [
        "X_train,X_test,y_train,y_test,vocab_size,tokenizer =  prepare_data(letter_data_file,json_flag=False)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of data 8675 \n",
            "Example from dataset\n",
            "  http   www youtube com watch v qsxhcwe krw http \n",
            "For type INFJ\n",
            "The vocabulary size is  121112 \n",
            "The maximum post length is 956 \n",
            "Found 121112 unique tokens.\n",
            "Example from tokenized X \n",
            "[11, 30, 31, 12, 26, 27, 11, 395, 451, 12, 451, 9101, 151, 84, 56, 293, 11, 30, 31, 12, 26, 27, 11809, 227, 651, 11810, 552, 1651, 243, 11, 30, 31, 12, 26, 27, 5690, 35, 1318, 143, 35, 11, 30, 31, 12, 26, 27, 11, 30, 31, 12]\n",
            "Shape of data tensor after tokenization and padding: (8675, 956)\n",
            "Vocabulary Size 121113 \n",
            "Number of examples in train set 6940 \n",
            "The shape of X training tensor (6940,956) \n",
            "The shape of y training tensor (6940) \n",
            "Number of examples in test set 1735 \n",
            "The shape of X test tensor (1735,956)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwikA2I6iOK9",
        "colab_type": "text"
      },
      "source": [
        "## Naive Bayes "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJNp_d6UiG0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Try for the first class as I-E\n",
        "# I-> 1 and E-> 0 \n",
        "model = MultinomialNB()\n",
        "kfolds = StratifiedKFold(n_splits=n_splits, shuffle=shuffle_flag, random_state=seed)\n",
        "\n",
        "results_nb = cross_validate(model, X_train_cnt,y_trains[:,0], cv=kfolds, \n",
        "                          scoring=scoring, n_jobs=-1)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ggy4CadCiTgQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "2134e017-bb1e-4e5e-b628-dc9693ba9375"
      },
      "source": [
        "print(\"CV Accuracy: {:0.4f} (+/- {:0.4f})\".format(np.mean(results_nb['test_acc']),\n",
        "                                                          np.std(results_nb['test_acc'])))\n",
        "\n",
        "print(\"CV F1: {:0.4f} (+/- {:0.4f})\".format(np.mean(results_nb['test_f1_micro']),\n",
        "                                                          np.std(results_nb['test_f1_micro'])))\n",
        "\n",
        "print(\"CV Logloss: {:0.4f} (+/- {:0.4f})\".format(np.mean(-1*results_nb['test_neg_log_loss']),\n",
        "                                                          np.std(-1*results_nb['test_neg_log_loss'])))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CV Accuracy: 0.7183 (+/- 0.0116)\n",
            "CV F1: 0.7183 (+/- 0.0116)\n",
            "CV Logloss: 2.3598 (+/- 0.1135)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgh4X4TmiVwP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "8f3b6c85-578e-445b-9ee9-2bbade164863"
      },
      "source": [
        "# fit 4 models \n",
        "model_names = ['IE','SN','TF', 'JP']\n",
        "\n",
        "for i in range(4):\n",
        "    print(\"Model for classes %s \" %(model_names[i]))\n",
        "    model = MultinomialNB()\n",
        "    model.fit(X_train_cnt,y_trains[:,i])\n",
        "    y_preds = model.predict(X_test_cnt)\n",
        "    accuracy = balanced_accuracy_score(y_tests[:,i],y_preds)\n",
        "    print(\"Model :\", model_names[i], \" Accuracy for test data {:0.4f}\".format(accuracy))\n"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model for classes IE \n",
            "Model : IE  Accuracy for test data 0.6666\n",
            "Model for classes SN \n",
            "Model : SN  Accuracy for test data 0.6770\n",
            "Model for classes TF \n",
            "Model : TF  Accuracy for test data 0.7867\n",
            "Model for classes JP \n",
            "Model : JP  Accuracy for test data 0.6458\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvcT9rcJjjio",
        "colab_type": "text"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-Sd95fmir4S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test for IE class\n",
        "# I-> 1 and E->0\n",
        "model_lr = LogisticRegression(solver = 'sag',multi_class='multinomial',max_iter=400,class_weight=\"balanced\", C=0.005)\n",
        "kfolds = StratifiedKFold(n_splits=n_splits, shuffle=shuffle_flag, random_state=seed)\n",
        "\n",
        "results_logrec = cross_validate(model_lr, X_train_cnt,y_trains[:,0], cv=kfolds, \n",
        "                          scoring=scoring, n_jobs=-1)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnBbOaLkjq6e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "b2141087-836c-4375-c201-feb2382a8ce0"
      },
      "source": [
        "print(\"CV Accuracy: {:0.4f} (+/- {:0.4f})\".format(np.mean(results_logrec['test_acc']),\n",
        "                                                          np.std(results_logrec['test_acc'])))\n",
        "\n",
        "print(\"CV F1: {:0.4f} (+/- {:0.4f})\".format(np.mean(results_logrec['test_f1_micro']),\n",
        "                                                          np.std(results_logrec['test_f1_micro'])))\n",
        "\n",
        "print(\"CV Logloss: {:0.4f} (+/- {:0.4f})\".format(np.mean(-1*results_logrec['test_neg_log_loss']),\n",
        "                                                          np.std(-1*results_logrec['test_neg_log_loss'])))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CV Accuracy: 0.7303 (+/- 0.0085)\n",
            "CV F1: 0.7303 (+/- 0.0085)\n",
            "CV Logloss: 0.5591 (+/- 0.0186)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrIPR32XjsmK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "484e40b8-1040-423f-8f05-a384e1ebba80"
      },
      "source": [
        "# fit 4 models \n",
        "model_names = ['IE','SN','TF', 'JP']\n",
        "\n",
        "for i in range(4):\n",
        "    print(\"Model for classes %s \" %(model_names[i]))\n",
        "    model = LogisticRegression(solver = 'sag',multi_class='multinomial',max_iter=400,class_weight=\"balanced\", C=0.005) \n",
        "\n",
        "    model.fit(X_train_cnt,y_trains[:,i])\n",
        "    y_preds = model.predict(X_test_cnt)\n",
        "    accuracy = balanced_accuracy_score(y_tests[:,i],y_preds)\n",
        "    print(\"Model :\", model_names[i], \" Accuracy for test data {:0.4f}\".format(accuracy))\n",
        "\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model for classes IE \n",
            "Model : IE  Accuracy for test data 0.6769\n",
            "Model for classes SN \n",
            "Model : SN  Accuracy for test data 0.6556\n",
            "Model for classes TF \n",
            "Model : TF  Accuracy for test data 0.7989\n",
            "Model for classes JP \n",
            "Model : JP  Accuracy for test data 0.6558\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3wgwgsHkPOy",
        "colab_type": "text"
      },
      "source": [
        "## Find overall Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQsCWlB3iqG-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "c1fb8bd4-0d87-48a7-82fb-7c771e4031f8"
      },
      "source": [
        "# fit 4 models \n",
        "model_names = ['IE','SN','TF', 'JP']\n",
        "y_preds_all = []\n",
        "\n",
        "model = LogisticRegression(solver = 'sag',multi_class='multinomial',max_iter=400,class_weight=\"balanced\", C=0.005) \n",
        "for i in range(4):\n",
        "    model.fit(X_train_cnt,y_trains[:,i])\n",
        "    y_preds = model.predict(X_test_cnt)\n",
        "    y_preds_all.append(y_preds)\n",
        "    accuracy = balanced_accuracy_score(y_tests[:,i],y_preds)\n",
        "    print(\"Model :\", model_names[i], \" Accuracy for test data {:0.4f}\".format(accuracy))\n"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model : IE  Accuracy for test data 0.6769\n",
            "Model : SN  Accuracy for test data 0.6556\n",
            "Model : TF  Accuracy for test data 0.7989\n",
            "Model : JP  Accuracy for test data 0.6558\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsuvf7mBrq51",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2fcbbe87-fa88-4d5c-b3f9-9a65d99e55fd"
      },
      "source": [
        "overall_accuracy(y_preds_all,y_tests)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3273775216138329"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHkMBiCJ50iv",
        "colab_type": "text"
      },
      "source": [
        "## Deep Learning with Tfidf "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_xrqYFOlCXU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define baseline model\n",
        "def baseline_model(input_dim,output_dim,hidden_dim):\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(hidden_dim, input_dim=input_dim, activation='relu'))\n",
        "\tmodel.add(Dense(output_dim, activation='sigmoid'))\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\treturn model"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQu4xep46LIT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "94e7c4ee-b95f-4efc-dcad-a77efb837fdc"
      },
      "source": [
        "input_dim = X_train_cnt.shape[1]\n",
        "output_dim = 1 \n",
        "hidden_dim = 50 \n",
        "model = baseline_model(input_dim,output_dim,hidden_dim)\n",
        "\n",
        "history = model.fit(X_train_cnt, y_trains[:,0],\n",
        "                    epochs=10,\n",
        "                    verbose=False,\n",
        "                    batch_size=10,\n",
        "                    validation_split=0.1)\n",
        "loss, accuracy = model.evaluate(X_train_cnt,  y_trains[:,0], verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test_cnt,  y_tests[:,0], verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy: 0.9764\n",
            "Testing Accuracy:  0.7620\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKOaTfON8BMR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "7430dc03-28e0-4e65-85b0-1d0a0f333aef"
      },
      "source": [
        "# fit 4 models \n",
        "model_names = ['IE','SN','TF', 'JP']\n",
        "y_preds_all = []\n",
        "input_dim = X_train_cnt.shape[1]\n",
        "output_dim = 1 \n",
        "hidden_dim = 50 \n",
        "\n",
        "for i in range(4):\n",
        "    #print(\"Predicting for model %s \" %(model_names[i]))\n",
        "    model = baseline_model(input_dim,output_dim,hidden_dim)\n",
        "    model.fit(X_train_cnt,y_trains[:,i],\n",
        "                    epochs=10,\n",
        "                    verbose=False,\n",
        "                    batch_size=50)\n",
        "    y_preds = model.predict(X_test_cnt)\n",
        "    y_preds = y_preds.reshape((1735,))\n",
        "    y_preds_all.append(y_preds)\n",
        "    loss, accuracy = model.evaluate(X_test_cnt, y_tests[:,i], verbose=False)\n",
        "    print(\"Model :\", model_names[i], \" Accuracy for test data {:0.4f}\".format(accuracy))\n"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model : IE  Accuracy for test data 0.7758\n",
            "Model : SN  Accuracy for test data 0.8571\n",
            "Model : TF  Accuracy for test data 0.7873\n",
            "Model : JP  Accuracy for test data 0.6605\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onC28qmn7Tr_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "27bc160d-177a-41d9-fe0f-2df5e2758782"
      },
      "source": [
        "overall_accuracy(y_preds_all,y_tests)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.35504322766570606"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjB3QPdbFDZD",
        "colab_type": "text"
      },
      "source": [
        "# Using Word Embeddings \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKaKOHDOE_--",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "79600629-a55d-4dfe-d7ee-6a0d4d587945"
      },
      "source": [
        "# get the embeddings \n",
        "! wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "folder = 'glove_folder'\n",
        "with zipfile.ZipFile(\"glove.6B.zip\",\"r\") as zip_ref:\n",
        "    zip_ref.extractall(folder)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-23 09:11:35--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-06-23 09:11:35--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-06-23 09:11:35--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.08MB/s    in 6m 27s  \n",
            "\n",
            "2020-06-23 09:18:02 (2.13 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1gdO4y0FUQq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from https://realpython.com/python-keras-text-classification/ \n",
        "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
        "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "    with open(filepath) as f:\n",
        "        for line in f:\n",
        "            word, *vector = line.split()\n",
        "            if word in word_index:\n",
        "                idx = word_index[word] \n",
        "                embedding_matrix[idx] = np.array(\n",
        "                    vector, dtype=np.float32)[:embedding_dim]\n",
        "\n",
        "    return embedding_matrix"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ui-IcfApFejh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dim = 50\n",
        "embedding_matrix = create_embedding_matrix(\n",
        "     'glove_folder/glove.6B.50d.txt',\n",
        "     tokenizer.word_index, embedding_dim)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nug8xmFgFgoZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def simple_embedding_model(input_dim, hidden_dim, input_length, output_dim):\n",
        "  model = Sequential()\n",
        "  model.add(layers.Embedding(input_dim=input_dim, \n",
        "                            output_dim=hidden_dim, \n",
        "                            input_length=input_length))\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(hidden_dim, activation='relu'))\n",
        "  model.add(Dense(output_dim, activation='softmax'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  #model.summary()\n",
        "  return model "
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqP7CyKA8KM8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "62d1c5f2-4656-4661-a758-642925247cb5"
      },
      "source": [
        "# fit 4 models \n",
        "embedding_dim = 50\n",
        "\n",
        "model_names = ['IE','SN','TF', 'JP']\n",
        "y_preds_all = []\n",
        "\n",
        "for i in range(4):\n",
        "    model = simple_embedding_model(input_dim=vocab_size,\n",
        "                               hidden_dim=50,\n",
        "                               input_length = X_train.shape[1],\n",
        "                               output_dim = 1)\n",
        "    model.fit(X_train,y_trains[:,i],\n",
        "                    epochs=5,\n",
        "                    verbose=False,\n",
        "                    batch_size=50,\n",
        "              validation_split=0.1)\n",
        "    y_preds = model.predict(X_test)\n",
        "    y_preds = y_preds.reshape((1735,))\n",
        "    y_preds_all.append(y_preds)\n",
        "    loss, accuracy = model.evaluate(X_test, y_tests[:,i], verbose=False)\n",
        "    print(\"Model :\", model_names[i], \" Accuracy for test data {:0.4f}\".format(accuracy))\n"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model : IE  Accuracy for test data 0.7625\n",
            "Model : SN  Accuracy for test data 0.1262\n",
            "Model : TF  Accuracy for test data 0.4697\n",
            "Model : JP  Accuracy for test data 0.3671\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLLtJkJOAp8R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f7ab449d-bc90-411e-86ac-e21ec06af343"
      },
      "source": [
        "overall_accuracy(y_preds_all,y_tests)"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.022478386167146973"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqcDtPF4cfET",
        "colab_type": "text"
      },
      "source": [
        "## LSTM + Glove Embedding \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnf2uYYnciV2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lstm_model_embedding(input_dim,\n",
        "              hidden_dim,\n",
        "              embedding_matrix, \n",
        "              input_length,\n",
        "              output_dim, \n",
        "              dropout = 0.2,\n",
        "              learning_rate=0.01):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(input_dim,\n",
        "                      hidden_dim, \n",
        "                      weights=[embedding_matrix], \n",
        "                      input_length=input_length,\n",
        "                      mask_zero=True, \n",
        "                      trainable=False))\n",
        "  model.add(LSTM(hidden_dim, \n",
        "                dropout=dropout, \n",
        "                recurrent_dropout=dropout, \n",
        "                activation='sigmoid',\n",
        "                kernel_initializer='zeros'))\n",
        "  model.add(Dense(output_dim, activation='sigmoid'))\n",
        "  optimizer = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffDLQdThcxCi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c17adc00-9f77-4020-f4de-821c8a1f841e"
      },
      "source": [
        "hidden_dim = 50 \n",
        "\n",
        "model_names = ['IE','SN','TF', 'JP']\n",
        "y_preds_all = []\n",
        "\n",
        "for i in range(4):\n",
        "    model = lstm_model_embedding(vocab_size,\n",
        "                  hidden_dim,\n",
        "                  embedding_matrix,\n",
        "                  X_train.shape[1],\n",
        "                  1)\n",
        "\n",
        "    model.fit(X_train,y_trains[:,i],\n",
        "                    epochs=10,\n",
        "                    verbose=False,\n",
        "                    batch_size=50,\n",
        "              validation_split=0.1)\n",
        "    y_preds = model.predict(X_test)\n",
        "    y_preds = y_preds.reshape((1735,))\n",
        "    y_preds_all.append(y_preds)\n",
        "    loss, accuracy = model.evaluate(X_test, y_tests[:,i], verbose=False)\n",
        "    print(\"Model :\", model_names[i], \" Accuracy for test data {:0.4f}\".format(accuracy))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model : IE  Accuracy for test data 0.7620\n",
            "Model : SN  Accuracy for test data 0.8738\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUnIScLQaDuJ",
        "colab_type": "text"
      },
      "source": [
        "## Bert Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yE46Xj9AaGBS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "377faa45-24ed-49be-9d49-d8929b48ff91"
      },
      "source": [
        "# install ktrain\n",
        "!pip3 install ktrain"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ktrain in /usr/local/lib/python3.6/dist-packages (0.16.3)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from ktrain) (3.2.2)\n",
            "Requirement already satisfied: keras-bert>=0.81.0 in /usr/local/lib/python3.6/dist-packages (from ktrain) (0.84.0)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from ktrain) (1.0.5)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.6/dist-packages (from ktrain) (0.42.1)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.6/dist-packages (from ktrain) (0.0.12)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from ktrain) (0.15.1)\n",
            "Requirement already satisfied: transformers>=2.11.0 in /usr/local/lib/python3.6/dist-packages (from ktrain) (2.11.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from ktrain) (20.4)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.6/dist-packages (from ktrain) (1.0.8)\n",
            "Requirement already satisfied: syntok in /usr/local/lib/python3.6/dist-packages (from ktrain) (1.3.1)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.6/dist-packages (from ktrain) (2.1.0)\n",
            "Requirement already satisfied: networkx>=2.3 in /usr/local/lib/python3.6/dist-packages (from ktrain) (2.4)\n",
            "Requirement already satisfied: bokeh in /usr/local/lib/python3.6/dist-packages (from ktrain) (1.4.0)\n",
            "Requirement already satisfied: tensorflow==2.1.0 in /usr/local/lib/python3.6/dist-packages (from ktrain) (2.1.0)\n",
            "Requirement already satisfied: fastprogress>=0.1.21 in /usr/local/lib/python3.6/dist-packages (from ktrain) (0.2.3)\n",
            "Requirement already satisfied: whoosh in /usr/local/lib/python3.6/dist-packages (from ktrain) (2.7.4)\n",
            "Requirement already satisfied: cchardet==2.1.5 in /usr/local/lib/python3.6/dist-packages (from ktrain) (2.1.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from ktrain) (2.23.0)\n",
            "Requirement already satisfied: scikit-learn==0.21.3 in /usr/local/lib/python3.6/dist-packages (from ktrain) (0.21.3)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from ktrain) (5.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->ktrain) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->ktrain) (1.18.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->ktrain) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->ktrain) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->ktrain) (0.10.0)\n",
            "Requirement already satisfied: keras-transformer>=0.37.0 in /usr/local/lib/python3.6/dist-packages (from keras-bert>=0.81.0->ktrain) (0.37.0)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-bert>=0.81.0->ktrain) (2.3.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=1.0.1->ktrain) (2018.9)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers>=2.11.0->ktrain) (0.1.91)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers>=2.11.0->ktrain) (0.0.43)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.11.0->ktrain) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.11.0->ktrain) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.11.0->ktrain) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=2.11.0->ktrain) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.11.0->ktrain) (3.0.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->ktrain) (1.12.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->ktrain) (0.16.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->ktrain) (0.9.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->ktrain) (2.3)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->ktrain) (0.22.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->ktrain) (3.10.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->ktrain) (1.1.0)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->ktrain) (19.3.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->ktrain) (1.12.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->ktrain) (0.3.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.3->ktrain) (4.4.2)\n",
            "Requirement already satisfied: Jinja2>=2.7 in /usr/local/lib/python3.6/dist-packages (from bokeh->ktrain) (2.11.2)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from bokeh->ktrain) (3.13)\n",
            "Requirement already satisfied: pillow>=4.0 in /usr/local/lib/python3.6/dist-packages (from bokeh->ktrain) (7.0.0)\n",
            "Requirement already satisfied: tornado>=4.3 in /usr/local/lib/python3.6/dist-packages (from bokeh->ktrain) (4.5.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0->ktrain) (1.29.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0->ktrain) (0.34.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0->ktrain) (0.2.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0->ktrain) (0.2.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0->ktrain) (2.1.0)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0->ktrain) (1.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0->ktrain) (3.2.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0->ktrain) (1.0.8)\n",
            "Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0->ktrain) (2.1.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0->ktrain) (1.1.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0->ktrain) (0.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->ktrain) (2020.4.5.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->ktrain) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->ktrain) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->ktrain) (1.24.3)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (0.8.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (1.0.18)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (4.3.3)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (2.1.3)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (47.3.1)\n",
            "Requirement already satisfied: keras-layer-normalization>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.37.0->keras-bert>=0.81.0->ktrain) (0.14.0)\n",
            "Requirement already satisfied: keras-embed-sim>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.37.0->keras-bert>=0.81.0->ktrain) (0.7.0)\n",
            "Requirement already satisfied: keras-pos-embd>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.37.0->keras-bert>=0.81.0->ktrain) (0.11.0)\n",
            "Requirement already satisfied: keras-position-wise-feed-forward>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.37.0->keras-bert>=0.81.0->ktrain) (0.6.0)\n",
            "Requirement already satisfied: keras-multi-head>=0.27.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.37.0->keras-bert>=0.81.0->ktrain) (0.27.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert>=0.81.0->ktrain) (2.10.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.11.0->ktrain) (7.1.2)\n",
            "Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow-datasets->ktrain) (1.52.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.7->bokeh->ktrain) (1.1.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->ktrain) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->ktrain) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->ktrain) (0.4.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->ktrain) (1.17.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->ktrain) (0.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ktrain) (0.2.4)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->ktrain) (0.2.0)\n",
            "Requirement already satisfied: keras-self-attention==0.46.0 in /usr/local/lib/python3.6/dist-packages (from keras-multi-head>=0.27.0->keras-transformer>=0.37.0->keras-bert>=0.81.0->ktrain) (0.46.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->ktrain) (1.6.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->ktrain) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->ktrain) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->ktrain) (4.1.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->ktrain) (4.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->ktrain) (3.1.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->ktrain) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->ktrain) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0-NBE4naKZn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import ktrain\n",
        "from ktrain import text"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSXcfk-RazSC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_data_bert(data_file,col_name,json_flag=False):\n",
        "    if json_flag:\n",
        "        dataset = read_dataset(data_file)\n",
        "        posts = list(dataset['posts'])\n",
        "        labels = list(dataset['types'])\n",
        "        print(\"Not implemented for dataframe\")\n",
        "        return \n",
        "    else:\n",
        "        dataset = pd.read_csv(data_file)\n",
        "\n",
        "    #return posts, labels\n",
        "    (X_train, y_train), (X_test, y_test), preproc = ktrain.text.texts_from_df(dataset, \n",
        "                                                                   'posts', \n",
        "                                                                   label_columns=[col_name],\n",
        "                                                                   maxlen=500, \n",
        "                                                                   preprocess_mode='bert')\n",
        "    return (X_train, y_train), (X_test, y_test), preproc "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUWTiWK9XmrL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "46de0685-8773-40f8-d0c0-d159656323f4"
      },
      "source": [
        "train,test,preproc = prepare_data_bert(letter_data_file,'I-E')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "preprocessing train...\n",
            "language: en\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "done."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Is Multi-Label? False\n",
            "preprocessing test...\n",
            "language: en\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "done."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9oWnTuQaCCQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "9bf3508d-ef0d-4fdc-ca2f-50e94f9a04a9"
      },
      "source": [
        "model = text.text_classifier('bert', train, preproc=preproc)\n",
        "learner = ktrain.get_learner(model,train_data=train, batch_size=3)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Is Multi-Label? False\n",
            "maxlen is 500\n",
            "done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LngyMHgWdBKD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "0ebb8a24-9268-463d-9611-7ebbf131329b"
      },
      "source": [
        "learner.fit_onecycle(2e-5, 1)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "begin training using onecycle policy with max lr of 2e-05...\n",
            "Train on 7807 samples\n",
            "  75/7807 [..............................] - ETA: 13:29:52 - loss: 0.7869 - accuracy: 0.7361"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-0cb58dd81a6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_onecycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ktrain/core.py\u001b[0m in \u001b[0;36mfit_onecycle\u001b[0;34m(self, lr, epochs, checkpoint_folder, cycle_momentum, max_momentum, min_momentum, verbose, class_weight, callbacks)\u001b[0m\n\u001b[1;32m    737\u001b[0m         hist = self.fit(lr, epochs, early_stopping=None,\n\u001b[1;32m    738\u001b[0m                         \u001b[0mcheckpoint_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_folder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m                         verbose=verbose, class_weight=class_weight, callbacks=kcallbacks)\n\u001b[0m\u001b[1;32m    740\u001b[0m         \u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'iterations'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'iterations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ktrain/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, lr, n_cycles, cycle_len, cycle_mult, lr_decay, checkpoint_folder, early_stopping, verbose, class_weight, callbacks)\u001b[0m\n\u001b[1;32m   1004\u001b[0m                                   \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m                                   \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m                                   callbacks=kcallbacks)\n\u001b[0m\u001b[1;32m   1007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msgdr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msgdr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgcTjfBZW1S9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}