{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains preprocessing steps for MBTI dataset. \n",
    "\n",
    "The Myers Briggs Type Indicator (or MBTI for short) is a personality type system that divides everyone into 16 distinct personality types across 4 axis:\n",
    "* Introversion (I) – Extroversion (E)\n",
    "* Intuition (N) – Sensing (S)\n",
    "* Thinking (T) – Feeling (F)\n",
    "* Judging (J) – Perceiving (P)\n",
    "\n",
    "In the dataset, there are 8600 rows of data. Each row contains a person's MBTI personality class and the last 50 things that he/she posted in PersonalityCafe Forum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import requests\n",
    "from lxml.html import fromstring\n",
    "import re \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import json \n",
    "import numpy as np\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno -2] Name or\n",
      "[nltk_data]     service not known>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [Errno -2] Name or\n",
      "[nltk_data]     service not known>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constant\n",
    "# define paths, constants etc. \n",
    "datadir = \"../dataset/\"\n",
    "datafile = \"../dataset/mbti-type/mbti_1.csv\"\n",
    "HTTP = [\"http://\", \"https://\", \".com\", \"www.\"]\n",
    "IMAGE = [\".jpg\",\".png\", \".gif\"]\n",
    "EMOJI = [\":D\",\":)\",\":(\",\"D:\",\":o\"]\n",
    "LINK = r'http\\S+'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open dataset folder into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8675 number of data \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...\n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...\n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
       "4  ENTJ  'You're fired.|||That's another silly misconce..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Opening dataset as pandas dataframe \n",
    "df = pd.read_csv(datafile)\n",
    "print(\"There are %d number of data \"  %len(df))\n",
    "# Looking the first 5 elements \n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape (8675,2)\n"
     ]
    }
   ],
   "source": [
    "# copy the original to use later\n",
    "df_copy = df.copy()\n",
    "# split the posts into array \n",
    "df_copy['posts'] = df_copy['posts'].apply(lambda x: x.split(\"|||\"))\n",
    "print(\"The shape (%d,%d)\" %(df_copy.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'http://www.youtube.com/watch?v=qsXHcwe3krw\",\n",
       " 'http://41.media.tumblr.com/tumblr_lfouy03PMA1qa1rooo1_500.jpg',\n",
       " 'enfp and intj moments  https://www.youtube.com/watch?v=iz7lE1g4XM4  sportscenter not top ten plays  https://www.youtube.com/watch?v=uCdfze1etec  pranks',\n",
       " 'What has been the most life-changing experience in your life?',\n",
       " 'http://www.youtube.com/watch?v=vXZeYwwRDw8   http://www.youtube.com/watch?v=u8ejam5DP3E  On repeat for most of today.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the first five posts of the first user\n",
    "df_copy.loc[0].posts[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  What to do with links? -> Scrap links and get the title of the page \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOT_ALLOWED= \"Bilgi\" # when page is closed by Bilgi Teknolojileri bk. \n",
    "\n",
    "def replace_links_title(df_copy):\n",
    "    \"\"\" for each row checks if the post \n",
    "    is a link, if it's a link then request the page. \n",
    "    For alexxandra-.tumblr.com the function gives a parser error. \n",
    "    saves the preprocessed file into csv. \n",
    "    @df_copy, a dataframe, each row has a posts array \n",
    "    returns : None \n",
    "    \"\"\"\n",
    "    nbr_link = 0 \n",
    "    link_dict = dict()\n",
    "    for i,post in enumerate(df_copy.posts):\n",
    "        link_dict[i] = dict()\n",
    "        for j,p in enumerate(post):\n",
    "            if any(f in p for f in HTTP):\n",
    "                # get the page title \n",
    "                link = re.findall(LINK,  p)\n",
    "                if not (\"http://-alexxxandra-.tumblr.com/\" in link) and not(\"http://memearchive.net/memerial.net/fullsize/1370.jpg\" in link):\n",
    "                    if len(link)>0:\n",
    "                        for l in link: # if multiple links \n",
    "                            try:\n",
    "                                r = requests.get(l)\n",
    "                                r.raise_for_status()\n",
    "                                try : \n",
    "                                    tree = fromstring(r.content)\n",
    "                                    title = tree.findtext('.//title') \n",
    "                                    if title and not(\"Bilgi\" in title): \n",
    "                                        p = re.sub(\n",
    "                                            LINK, \n",
    "                                            title, \n",
    "                                            p) \n",
    "                                    link_dict[i][j] = p\n",
    "                                    nbr_link += 1\n",
    "                                except: \n",
    "                                    print(\"Error in from String for i: %d j: %d\", (i,j))\n",
    "                            except: \n",
    "                                print(\"Error for i: %d j:%d\" %(i,j))\n",
    "            post[j] = p                  \n",
    "        if i%50==0: \n",
    "            # save the results \n",
    "            print(\"Saving result for %d \" %(i))\n",
    "            df_copy.to_csv(\"backup_df.csv\")\n",
    "\n",
    "        df_copy.loc[i]['posts'] = post\n",
    "    print(\"Number of links in the whole data %d \" %(nbr_link))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization and Removing StopWords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ENFJ' 'ENFP' 'ENTJ' 'ENTP' 'ESFJ' 'ESFP' 'ESTJ' 'ESTP' 'INFJ' 'INFP'\n",
      " 'INTJ' 'INTP' 'ISFJ' 'ISFP' 'ISTJ' 'ISTP']\n",
      "['INFJ', 'ENTP', 'INTP', 'INTJ', 'ENTJ', 'ENFJ', 'INFP', 'ENFP', 'ISFP', 'ISTP', 'ISFJ', 'ISTJ', 'ESTP', 'ESFP', 'ESTJ', 'ESFJ']\n"
     ]
    }
   ],
   "source": [
    "# Labels for types\n",
    "lab_encoder = LabelEncoder().fit(list(df.type.unique()))\n",
    "classes=lab_encoder.inverse_transform(range(16))\n",
    "print(classes)\n",
    "print(['INFJ', 'ENTP', 'INTP', 'INTJ', 'ENTJ', 'ENFJ', 'INFP', 'ENFP', 'ISFP', 'ISTP', 'ISFJ', 'ISTJ', 'ESTP', 'ESFP', 'ESTJ', 'ESFJ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words \n",
    "lemmatiser = WordNetLemmatizer()\n",
    "def preprocess_data(df_copy, \n",
    "                    punc_remove=True,\n",
    "                    link_remove=True,\n",
    "                    only_letters=True,\n",
    "                    type_remove=True\n",
    "                   ):\n",
    "    stopWordsEng = stopwords.words(\"english\")\n",
    "    post_list = []\n",
    "    label_list = []\n",
    "    exclusions = '|'.join([f.lower() for f in list(df.type.unique())])\n",
    "    for row in tqdm(df_copy.iterrows()):\n",
    "        posts = row[1].posts\n",
    "        temp_post = \"\"\n",
    "        for p in posts:\n",
    "            # change remaining links as LNK\n",
    "            temp = p.lower()\n",
    "            if link_remove: temp = re.sub(LINK, 'link',temp)\n",
    "            # change type names as TYP \n",
    "            if type_remove: temp = re.sub(exclusions, 'type', temp)\n",
    "            # chose only letters \n",
    "            if only_letters: temp = re.sub(\"[^a-zA-Z]\", \" \", temp)\n",
    "            # remove punctuations \n",
    "            if punc_remove: temp = re.sub(' +', ' ', temp)\n",
    "            # remove stopwords and lemmatize \n",
    "            temp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(' ') if w not in stopWordsEng])\n",
    "            temp_post += \" \" +temp\n",
    "        post_list.append(temp_post)\n",
    "        label_list.append(lab_encoder.transform([row[1].type])[0])\n",
    "    return post_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8675it [00:57, 149.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# First replace links \n",
    "# replace_links_title(df_copy)\n",
    "# apply remaining preprocess \n",
    "posts_all, label_all = preprocess_data(df_copy)\n",
    "# save the preprocessed file \n",
    "data = dict()\n",
    "data['posts'] = posts_all\n",
    "data['types'] = [int(l) for l in label_all] \n",
    "filename = join(datadir,\"preprocessed_data_all.json\")\n",
    "with open(filename, \"w+\") as fp:\n",
    "    json.dump(data,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8675it [00:47, 182.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# only lemmatize\n",
    "posts_none, label_none = preprocess_data(df_copy, False,False,False,False)\n",
    "# save the preprocessed file \n",
    "data = dict()\n",
    "data['posts'] = posts_none\n",
    "data['types'] = [int(l) for l in label_none] \n",
    "filename = join(datadir,\"preprocessed_data_none.json\")\n",
    "with open(filename, \"w+\") as fp:\n",
    "    json.dump(data,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8675it [00:48, 177.98it/s]\n",
      "8675it [00:46, 184.88it/s]\n",
      "8675it [01:05, 133.45it/s]\n",
      "8675it [00:49, 176.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# For the ablation study different version added \n",
    "nbr_feature = 4 \n",
    "# experiment names related to the removed features \n",
    "# For punc  -> only punctuations removed \n",
    "#     links -> only links replaced \n",
    "#     letters-> only non-letters removed\n",
    "#     type  -> only type names removed \n",
    "exp_names = [\"punc\",\"links\",\"letters\",\"type\"]\n",
    "for i in range(nbr_feature):\n",
    "    posts, labels = preprocess_data(df_copy, \n",
    "                                    (exp_names[i]==\"punc\"),\n",
    "                                    (exp_names[i]==\"links\"),\n",
    "                                    (exp_names[i]==\"letters\"),\n",
    "                                    (exp_names[i]==\"type\"))\n",
    "    data = dict()\n",
    "    data['posts'] = posts\n",
    "    data['types'] = [int(l) for l in labels] \n",
    "    filename = join(datadir,\"preprocessed_data_\"+exp_names[i]+\".json\")\n",
    "    with open(filename, \"w+\") as fp:\n",
    "        json.dump(data,fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split classes as 4 dichotomies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_4classes(data_file):\n",
    "    with open(data_file) as fp:\n",
    "        data = json.load(fp)\n",
    "    data['posts']  = [p.lower() for p in data['posts']]   \n",
    "    # change the class names as originals  \n",
    "    for i,l in enumerate(data['types']):\n",
    "        cls = lab_encoder.inverse_transform([l])\n",
    "        data['types'][i] = cls[0]\n",
    "    #print(data['types'][0:10])\n",
    "    # construct a dataframe \n",
    "    df_preprocessed = pd.DataFrame.from_dict(data)\n",
    "    # print(df_preprocessed.head())\n",
    "    # check if class names are true \n",
    "    # print(sum(df_preprocessed.types == df.type)) \n",
    "    # Create column for E/I \n",
    "    # if types start I -> 1, if starts with E->0 \n",
    "    df_preprocessed['I-E'] = np.zeros(len(df_preprocessed),dtype=int)\n",
    "    df_preprocessed['I-E'][df_preprocessed.types.str.startswith('I')] = 1 \n",
    "    # Create column for S-N \n",
    "    # if types start S -> 1, if starts with N->0 \n",
    "    df_preprocessed['S-N'] = np.zeros(len(df_preprocessed),dtype=int)\n",
    "    df_preprocessed['S-N'][df_preprocessed.types.str[1] == 'S'] = 1 \n",
    "    # Create column for T-F \n",
    "    # if types start T -> 1, if starts with F->0 \n",
    "    df_preprocessed['T-F'] = np.zeros(len(df_preprocessed),dtype=int)\n",
    "    df_preprocessed['T-F'][df_preprocessed.types.str[2] == 'T'] = 1 \n",
    "    # Create column for J-P \n",
    "    # if types start J -> 1, if starts with P->0 \n",
    "    df_preprocessed['J-P'] = np.zeros(len(df_preprocessed),dtype=int)\n",
    "    df_preprocessed['J-P'][df_preprocessed.types.str[3] == 'J'] = 1 \n",
    "    return df_preprocessed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open preprocessed dataset \n",
    "# Labels for types\n",
    "lab_encoder = LabelEncoder().fit(list(df.type.unique()))\n",
    "classes=lab_encoder.inverse_transform(range(16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matmazel/miniconda3/envs/py3Torch/lib/python3.6/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/matmazel/miniconda3/envs/py3Torch/lib/python3.6/site-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/matmazel/miniconda3/envs/py3Torch/lib/python3.6/site-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/matmazel/miniconda3/envs/py3Torch/lib/python3.6/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "data_file = \"../dataset/preprocessed_data_letters.json\"\n",
    "df_preprocessed = make_4classes(data_file)\n",
    "df_preprocessed.head()\n",
    "# save the 4 dichotomies dataset \n",
    "df_preprocessed.to_csv(\"../dataset/preprocessed_data_letters_4class.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>posts</th>\n",
       "      <th>types</th>\n",
       "      <th>I-E</th>\n",
       "      <th>S-N</th>\n",
       "      <th>T-F</th>\n",
       "      <th>J-P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>http   www youtube com watch v qsxhcwe krw h...</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>finding lack post alarming  sex boring posit...</td>\n",
       "      <td>ENTP</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>good one          http   www youtube com wat...</td>\n",
       "      <td>INTP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>dear intp    enjoyed conversation day   esot...</td>\n",
       "      <td>INTJ</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>fired  another silly misconception  approach...</td>\n",
       "      <td>ENTJ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              posts types  I-E  \\\n",
       "0           0    http   www youtube com watch v qsxhcwe krw h...  INFJ    1   \n",
       "1           1    finding lack post alarming  sex boring posit...  ENTP    0   \n",
       "2           2    good one          http   www youtube com wat...  INTP    1   \n",
       "3           3    dear intp    enjoyed conversation day   esot...  INTJ    1   \n",
       "4           4    fired  another silly misconception  approach...  ENTJ    0   \n",
       "\n",
       "   S-N  T-F  J-P  \n",
       "0    0    0    1  \n",
       "1    0    1    0  \n",
       "2    0    1    0  \n",
       "3    0    1    1  \n",
       "4    0    1    1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the written file \n",
    "df_w = pd.read_csv('../dataset/preprocessed_data_letters_4class.json')\n",
    "df_w.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w[['I-E','S-N','T-F','J-P']].sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
